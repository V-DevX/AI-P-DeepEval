---
id: getting-started-llm-arena
title: LLM Arena Evaluation
sidebar_label: LLM Arena
---

Learn how to evaluate different versions of your LLM app using LLM Arena-as-a-Judge in `deepeval`, a comparison-based LLM eval.

## Overview

Instead of comparing LLM outputs using a single-output LLM-as-a-Judge method as seen in previous sections, you can also compare n-pairwise test cases to find the best version of your LLM app. This method although does not provide numerical scores, allows you to more reliably choose the "winning" LLM output for a given set of inputs and outputs.

**In this 5 min quickstart, you'll learn how to:**

- Setup an LLM arena
- Use Arena G-Eval to pick the best performing LLM app

## Prerequisites

- Install `deepeval`

## Setup LLM Arena

[Needa do here]

## Run Your First Arena Evals

[Needa do this, show compare() method (find in code)]

## Next Steps

[Use navigation cards to route them back to agents rags or chatbot, say that arena is just for picking winners]
[follow same format as before]
